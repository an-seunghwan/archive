## 1. 주성분 분석 

### 1. 개요

<img src="https://upload.wikimedia.org/wikipedia/commons/1/15/GaussianScatterPCA.png" width="400">

- 위 그림은 2차원 공간상에 분포해있는 2차원 데이터의 예시이다. 해당 데이터가 퍼져있는 전체적인 형태는 그림에 표시된 화살표 2개를 이용해 충분히 표현이 가능한데, 고차원 데이터에 대해서 이러한 화살표를 찾는 작업이 주성분 분석의 메인 아이디어라고 할 수 있다.
- 주어진 데이터는 고차원 축들을 따라서 일정한 크기의 변동(데이터가 퍼져있는 정도, 데이터가 표현하는 정보의 크기, 전체 변동, 분산)을 가진다. 주성분 분석은 주어진 전체 변동을 가장 잘 설명할 수 있는 여러 새로운 축(주성분 축)들을 만드는 방법을 말하고, 이러한 새로운 축의 개수는 주어진 데이터의 변수의 개수와 동일하다.
- 주성분 축은 주어진 데이터의 변수들의 선형결합을 구성하는 선형결합 계수들을 의미한다. 주어진 데이터를 10차원이라고 하고, 10개의 변수가 $x_1, x_2, \cdots, x_{10}$라고 할 때, 변수들에 대한 선형결합은 실수들로 구성된 선형결합 계수 $a_1, a_2, \cdots, a_{10}$들에 대해서 $$a_1x_1 + a_2x_2 + \cdots + a_{10}x_{10}$$을 의미한다. 이때, 10차원 벡터의 선형결합의 결과는 1차원 실수임을 기억하자.
- 주성분 축들은 서로 선형 연관성이 없으며, 선형 연관성이 없다는 것은 주성분 축에 사용된 선형결합 계수 벡터 2개를 내적하면 그 값이 0이 됨을 의미한다(이는 두 선형결합 계수 벡터가 서로 직교한다는 것과 동일). 이를 수식으로 표현하면, 2개의 주성분 축을 구성하는 선형결합 계수들을 각각 $(a_1, a_2, \cdots, a_{10})$, $(b_1, b_2, \cdots, b_{10})$이라고 할 때 두 벡터의 내적 $$a_1b_1 + a_2b_2 + \cdots + a_{10}b_{10} = 0$$이 된다.

#### 활용 예시: 서울집 구별 집 가격의 주성분 분석
- 데이터: 서울시 구별 집 가격 데이터
- 설명변수: 집의 넓이, 방의 개수, 지어진 연도, 도심지로부터의 거리, 집이 속한 구의 범죄율 등
- 관심변수: 집 가격

1. 고차원 설명변수들로 구성된 관측치들을 주성분 분석을 이용해 2차원의 저차원 데이터로 요약
2. 2차원 공간상에서 차원축소된 저차원 관측치들을 주성분 축을 따라서 시각화
3. 주성분의 값에 따라 관측치들의 집 가격이 어떻게 분포해 있는지를 확인
4. 주성분 축을 구성할 때 사용된 선형결합 계수를 활용하여, 각각의 설명변수들이 집 가격 형성에 미치는 영향(예. 양/음의 관계) 파악

### 2. 목적

#### 1. 설명할 수 있는 변동이 큰 축 탐색
- 주어진 데이터로부터 여러개의 주성분 축을 얻을 수 있는데, 각각의 주성분 축은 서로 다른 크기의 변동을 설명할 수 있다. 여기서 중요한 점은 각 주성분 축들이 설명할 수 있는 변동들의 합은 주어진 데이터가 가지는 전체 변동과 같아진다. 일반적으로 가장 큰 변동을 설명할 수 있는 주성분을 첫 번째 주성분이라고 부른다.
- 여기서 설명할 수 있는 변동이 가장 큰 축을 탐색하여 알아낸다면 해당 주성분 축은 다른 축에 비해 더 많은 정보의 크기를 표현할 수 있음을 의미한다. 
- 설명할 수 있는 변동이 큰 주성분 축을 알아낸다면 주어진 데이터의 변수들로부터 어떻게 구성되는지를 파악할 수 있게 된다. 따라서, 어떠한 방식으로 변수들을 조합(선형결합)하는 것이 가장 데이터를 효과적으로 설명할 수 있는지, 즉 많은 부분의 데이터 변동을 설명할 수 있는지 알 수 있다.

#### 2. 차원 축소
- 우리는 새롭게 만든 축, 즉 주성분 축들을 이용해 전체 변동의 크기를 최대한 유지하면서 고차원 공간에 속하는 관측된 데이터들을 새로운 저차원 공간의 표본(주성분)으로 변환할 수 있다. 

1. 여러 주성분 축 중에서, 설명할 수 있는 변동의 크기 순으로 나열하여 상위 크기의 변동을 가지는 주성분 축들을 선택 (이때, 해당 축들의 변동의 합은 데이터가 가지는 전체 변동의 크기와 비슷하도록 설정, 스크리 플롯, Scree plot)
2. 선택된 주성분 축들을 이용해 주어진 고차원 데이터를 선형결합을 통해 요약
3. 고차원 데이터를 선택된 주성분 축의 개수만큼의 차원의 데이터로 축소

#### 3. 주성분을 통한 데이터 해석
- 주어진 데이터의 상태에서는 잘 구분되지 않던 데이터가 주성분 분석을 이용해 차원축소를 수행하면 저차원 공간에서 쉽게 구분될 수 있어 같은 라벨을 갖는 데이터끼리 그룹지어 해석할 수 있다는 장점이 있다.

### 3. 구하는 방법
1. 주어진 데이터 변수들의 공분산 행렬을 구한다.
2. 공분산 행렬의 고유벡터와 고유값들을 계산 
   - 해당 고유벡터들은 서로 직교
   - 고유값의 크기 = 각 주성분에 의해 설명되는 분산의 크기
3. 고유값이 큰 순서대로 고유벡터들을 나열
4. 고유값이 가장 큰 고유벡터를 이용해 주어진 데이터를 선형결합 = 첫 번째 주성분
5. 두 번째로 고유값이 큰 고유벡터를 이용해 주어진 데이터를 선형결합 = 두 번째 주성분
6. 원하는 개수의 주성분(혹은 원하는 변동의 크기)을 얻을 때까지 반복

## 2. Restricted Boltzmann Machine (RBM)

### 1. 정의
- 주어진 데이터의 확률 분포를 학습할 수 있는 확률적 생성 인공 신경망 모형(generative stochastic artificial neural network)이라고 할 수 있다.

### 2. 구조
<img src="https://github.com/an-seunghwan/archive/blob/main/assets/rbm.png?raw=true" width="200">

1. 위의 그림에서 볼 수 있는 것처럼 RBM은 두 그룹의 유닛(unit)들로 구성되어 있다. 
    - 두 그룹 = visible units(그림의 왼쪽), hidden units(그림의 오른쪽)
    - visible units는 주어진 데이터들을 의미한다.
    - hidden units는 visible units들의 분포를 결정하는 숨겨진 값들을 의미한다. 
2. 각 그룹의 unit들은 이분 그래프(bipartite graph)로 연결되어 있다. 즉, visible units과 hidden units들은 서로 연결되어 있지만, 각 그룹 내에서는 내부적으로 서로 연결되어있지 않다.
3. visible units가 주어진 경우의 hidden units에 대한 조건부 확률을 최대화하는 방향으로 모형을 학습하게 된다. 즉, 저차원의 hidden units의 값에 따라서 고차원의 visible units들의 조건부 분포가 결정되므로 1) 고차원 데이터에 대한 차원축소를 수행할 수 있고, 2)해당 조건부 분포를 이용해 새로운 visible units들을 확률적으로 생성할 수 있다.

### 3. 수식
0. visible units와 hidden units가 모두 0 또는 1의 값만을 가지는 경우를 고려한다.
1. $i$번째 visible unit: $v_i$,$i=1,2,\cdots,n$
2. $j$번째 hidden unit: $h_j$,$j=1,2,\cdots,m$, (차원 축소: $n > m$)
3. 가중치 행렬: $W \in \mathbb{R}^{n \times m}$
    - $w_{ij}$: visible unit $w_i$와 hidden unit $h_j$의 연결에 해당하는 가중치
4. 편이(bias) 항:
    - $a_i$: $v_i$의 편이 항
    - $b_j$: $h_j$의 편이 항
5. 조건부 분포
    - RBM의 그래프 구조는 이분 그래프이므로, visible units가 주어진 경우에 hidden units들은 서로 연결되어 있지 않으므로 서로 독립이다. 마찬가지로, hidden units가 주어진 경우에 visible units들은 서로 독립이다.
    - $$P(v|h) = \prod_{i=1}^n P(v_i|h)$$
    - $$P(h|v) = \prod_{j=1}^m P(h_j|v)$$
6. visible unit의 분포
    - visible unit $v_i$는 0 또는 1의 값만을 가지므로 베르누이 분포를 따르는 확률변수이다.
    - $$v_i \sim \text{Ber}(P(v_i = 1|h)) = \text{Ber}(\sigma\left(a_i + \sum_{j=1}^m w_{ij}h_j\right))$$
    - 이때, $\sigma(x) = 1/(1 + \exp(-x))$.
7. hidden unit의 분포
    - hidden unit $h_j$는 0 또는 1의 값만을 가지므로 베르누이 분포를 따르는 확률변수이다.
    - $$h_j \sim \text{Ber}(P(h_j = 1|v)) = \text{Ber}(\sigma\left(b_j + \sum_{i=1}^n v_i w_{ij}\right))$$
    - 이때, $\sigma(x) = 1/(1 + \exp(-x))$.
8. 위의 조건부 분포를 이용해 sampling된 visible units의 값이 주어진 데이터의 값과 충분히 같아질 때까지 경사하강법을 이용해 가중치 행렬과 편이 항을 학습한다.