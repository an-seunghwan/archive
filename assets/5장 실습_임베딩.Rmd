---
title: "고차원 데이터 임베딩"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 고차원 연속형 데이터가 주어진 경우 PCA를 통해 변수만들기

### 1. 데이터 불러오기 (wine dataset)

```{r}
# install.packages("sysfonts")
library(sysfonts)

winedf <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"), header=T)
names(winedf) = c("Cvs","Alcohol","Malic acid","Ash","Alcalinity of ash", "Magnesium", "Total phenols", "Flavanoids", "Nonflavanoid phenols", "Proanthocyanins", "Color intensity", "Hue", "OD280/OD315 of diluted wines", "Proline")
head(winedf)
```

### 2. 데이터 시각화

각 class에 해당하는 관측치들이 주어진 변수들에 의해 서로 구분되지 않음을 확인할 수 있다.

```{r}
wineClasses = factor(winedf$Cvs) # class를 나타내는 factor

plot(main="Three Different Cultivars",
     winedf$'Alcohol',
     winedf$'Alcalinity of ash', 
     col = wineClasses)

plot(main="Three Different Cultivars",
     winedf$`Malic acid`,
     winedf$Magnesium, 
     col = wineClasses)

plot(main="Three Different Cultivars",
     winedf$Ash,
     winedf$Flavanoids, 
     col = wineClasses)
```

### 3. 주성분 분석

```{r}
winePCA <- prcomp(scale(winedf[,-1]))
summary(winePCA)
```

`Standard deviation`: 각 주성분이 설명하는 변동 크기 (표준 편차)

`Proportion of Variance`: 전체 변동에서 각 주성분이 설명하는 변동 비율

### 4. 주성분 분석 시각화

```{r}
pcaCharts <- function(x) {
  x.var <- x$sdev ^ 2
  x.pvar <- x.var/sum(x.var)
  
  par(mfrow=c(2,2), family='AppleGothic')
  plot(x.pvar,xlab="Principal component", ylab="각 주성분이 설명하는 변동의 비율", ylim=c(0,1), type='b')
  plot(cumsum(x.pvar), xlab="Principal component", ylab="각 주성분이 설명하는 변동의 누적 비율", ylim=c(0,1), type='b')
  screeplot(x)
  screeplot(x, type="l") 
  par(mfrow=c(1,1))
}

pcaCharts(winePCA)
```

### 5. 주성분 축 시각화

```{r}
biplot(winePCA, scale=0, cex=.7)
```

예를 들어, `Flavanoids`나 `Alcalinity of ash`와 같은 변수들은 첫 번째 주성분 축 (PC1)와 평행하므로 첫 번째 주성분에 많은 기여를 하였고,
`Ash`나 `Color intensity`와 같은 변수들은 첫 번째 주성분 축 (PC1)에 거의 수직이므로 큰 기여를 하지 않고, 대신 두 번째 주성분에 많은 기여를 하였다고 해석할 수 있다.

### 6. 주성분을 이용한 데이터 산점도 

```{r}
plot(main="Three Different Cultivars",
     winePCA$x[,1:2], 
     col = wineClasses)
```

첫 번째 주성분과 두 번째 주성분에 의해서 관측치들이 각 class별로 구분되는 것을 확인할 수 있다.

## 고차원 이진(범주형)데이터가 주어진 경우 RBM을 이용한 임베딩

### 1. MNIST 데이터 불러오기 및 시각화

```{r}
# install.packages("remotes")
# remotes::install_github("TimoMatzen/RBM")
library(RBM)

# 고차원 이진 데이터 MNIST를 불러온다.
data(MNIST)

MNIST$trainX[2, ][1:10]

# visible unit의 수 = 784 + 1 (bias 항)
length(MNIST$trainX[2, ])

# 1개의 훈련 데이터를 시각화
image(matrix(MNIST$trainX[2, ], nrow = 28), col = grey(seq(0, 1, length = 256)))
```

### 2. RBM 모형 학습

```{r}
# 훈련 데이터 저장
train <- MNIST$trainX

# RBM 모형을 적합
# n.hidden: bias 항을 제외한 hidden unit의 개수
modelRBM <- RBM(x = train, n.iter = 1000, n.hidden = 100, size.minibatch = 10)

# 학습된 RBM 모형의 가중치
head(modelRBM$trained.weights)

# visible unit의 수 x hidden unit의 수 
dim(modelRBM$trained.weights)
```

### 3. RBM 모형을 이용한 이미지 복원

```{r}
# MNIST 테스트 데이터
test <- matrix(MNIST$testX[6, ], nrow = 1)

# visible unit에 bias 항을 추가
V <- cbind(1, test[1,, drop = FALSE])
```

#### 1. hidden unit을 계산

```{r}
# sigmoid 함수를 이용해 hidden unit을 계산
# = visible unit이 주어진 경우, hidden unit이 1일 확률값
H <- 1/(1 + exp(-(V %*% modelRBM$trained.weights))) 

H[1:10]
length(H)
```

#### 2. visible unit을 복원

```{r}
# sigmoid 함수를 이용해 visible unit을 복원
# = hidden unit이 주어진 경우, visible unit이 1일 확률값
V.rec <- 1/(1 + exp(-(H %*% t(modelRBM$trained.weights)) ))

V.rec[1:10]
length(V.rec)
```

#### 3. 복원 결과 시각화

```{r}
par(mfrow = c(1,2), family='AppleGothic')
image(matrix(V[, -1], nrow = sqrt(ncol(test))), col = grey(seq(0, 1, length = 256)))
title(main = '원래 이미지', font.main = 4)
image(matrix(V.rec[, -1], nrow = sqrt(ncol(test))), col = grey(seq(0, 1, length = 256)))
title(main = '복원된 이미지', font.main = 4)
```

